{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d4d54b",
   "metadata": {},
   "source": [
    "## Spatial Coverage Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47805872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new table to analyse the spatial coverage\n",
    "df_sp = clean_nom_francais.copy()\n",
    "\n",
    "print(df_sp.head())\n",
    "\n",
    "# Make sure date/year exist\n",
    "df_sp[\"date\"]  = pd.to_datetime(df_sp[\"date\"], errors=\"coerce\")\n",
    "df_sp = df_sp.dropna(subset=[\"date\"])\n",
    "df_sp[\"year\"] = df_sp[\"date\"].dt.year\n",
    "\n",
    "# We ensure all columns contain only numerical values\n",
    "df_sp[\"point_n\"] = pd.to_numeric(df_sp[\"N¬∞ point\"], errors=\"coerce\")\n",
    "df_sp[\"N¬∞ passage\"] = pd.to_numeric(df_sp[\"N¬∞ passage\"], errors=\"coerce\")\n",
    "df_sp[\"TOT_AV_sV\"] = pd.to_numeric(df_sp[\"TOT_AV_sV\"], errors=\"coerce\")\n",
    "\n",
    "# We group the relevant indicators by year\n",
    "yearly = (\n",
    "    df_sp.groupby(\"year\", dropna=True)\n",
    "      .agg(transects=(\"Nom transect\", \"nunique\"),\n",
    "           observers=(\"Nom observateur\", \"nunique\"),\n",
    "           departments=(\"code d√©partement\", \"nunique\"),\n",
    "           point_visits=(\"Nom transect\", \"size\"),\n",
    "           richness=(\"ESPECE\", \"nunique\"),\n",
    "           total_counts=(\"TOT_AV_sV\", \"sum\"))\n",
    "      .reset_index()\n",
    "      .sort_values(\"year\")\n",
    ")\n",
    "yearly[\"birds_per_visit\"] = yearly[\"total_counts\"] / yearly[\"point_visits\"]\n",
    "\n",
    "# We'll use this to calculate the coverage ratio per year\n",
    "# To do that we take divide the unique (transect, point, pass) by the maximum number of (transects * points * passes) there can be (= transects * 10 * 2)\n",
    "df_sp[\"combo\"] = list(zip(df_sp[\"Nom transect\"], df_sp[\"point_n\"], df_sp[\"N¬∞ passage\"]))\n",
    "\n",
    "cov_y = (df_sp.dropna(subset=[\"year\"])\n",
    "           .groupby(\"year\")\n",
    "           .agg(obs_combos=(\"combo\", \"nunique\"),\n",
    "                max_passes=(\"N¬∞ passage\", \"max\")))\n",
    "cov_y = cov_y.join(yearly.set_index(\"year\")[[\"transects\"]])\n",
    "cov_y[\"max_combos\"] = cov_y[\"transects\"] * 10 * 2 \n",
    "cov_y[\"combo_coverage\"] = cov_y[\"obs_combos\"] / cov_y[\"max_combos\"]\n",
    "yearly = yearly.merge(cov_y[[\"combo_coverage\"]], left_on=\"year\", right_index=True, how=\"left\")\n",
    "\n",
    "\n",
    "# Coverage heatmap data \n",
    "Top_transects = 15 # Number of top transects by visits that will appear in the heatmap\n",
    "\n",
    "ded = (df_sp.dropna(subset=[\"point_n\", \"N¬∞ passage\"])\n",
    "         .drop_duplicates(subset=[\"Nom transect\", \"year\", \"point_n\", \"N¬∞ passage\"]))\n",
    "cov = (ded.groupby([\"Nom transect\", \"year\"])\n",
    "           .size()\n",
    "           .unstack(fill_value=0))\n",
    "if not cov.empty:\n",
    "    keep = df_sp[\"Nom transect\"].value_counts().head(Top_transects).index\n",
    "    cov = cov.loc[cov.index.intersection(keep)]\n",
    "\n",
    "\n",
    "# A) Transects sampled per year\n",
    "plt.figure()\n",
    "plt.plot(yearly[\"year\"], yearly[\"transects\"], marker=\"o\")\n",
    "plt.title(\"Transects sampled per year\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"# transects\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# B) Coverage ratio per year\n",
    "plt.figure()\n",
    "plt.plot(yearly[\"year\"], yearly[\"combo_coverage\"], marker=\"o\")\n",
    "plt.title(\"Point‚Äìpass coverage ratio per year\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Observed / Max combos\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# C) Coverage heatmap (unique point‚Äìpass combos per transect-year)\n",
    "plt.figure(figsize=(12, max(5, 0.3 * len(cov))))\n",
    "im = plt.imshow(cov.values, aspect=\"auto\")\n",
    "plt.colorbar(im, fraction=0.02, pad=0.02, label=\"# point‚Äìpass combos\")\n",
    "plt.yticks(range(len(cov.index)), cov.index)\n",
    "plt.xticks(range(len(cov.columns)), cov.columns, rotation=45)\n",
    "plt.title(\"Coverage heatmap ‚Äî unique (point, pass) combos per transect-year\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Transect\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdedb27",
   "metadata": {},
   "source": [
    "Let's analyse these three figures, that represent different aspects of the spatial coverage.\n",
    "\n",
    "**1) Transects sampled per year** \n",
    "\n",
    "Early years show fewer transects visited (around 41 in 2014), then a ramp-up through 2018 (around 65) and a rather stable plateau afterward (mostly between 62‚Äì65). So spatial coverage by number of transects expanded fast and then stabilized.\n",
    "\n",
    "\n",
    "**2) Point‚Äìpass coverage ratio per year** \n",
    "Although the line looks dramatic at first, the y-axis is very tight: from around 0.91 to 1.00. The only clear drop is between 2019 and 2020, and after that, the coverage rebounds to around 0.99 and remains high. These graph shows there is a near-complete coverage most years, with 2020 as the outlier.\n",
    "\n",
    "**3) Coverage heatmap (unique point√ópass combos by transect‚Äìyear, top-15 transects)**\n",
    "On the first few years, we can see some gaps (`Borelie`, `Boucle du Vauclin`) and then some isolated holes  (`H√¥tel des Plaisirs` around 2018‚Äì2019). Besides those, cells are almost always at the maximum (around 20 combos), which indicates a full pointxpass combo after 2016.\n",
    "\n",
    "**Overall**\n",
    "With the three figures, we can observe that there is a big increase in spatial coverage in the first years, followed by a stable coverage from 2018 onward. This sampling indicator looks strong and steady after the initial ramp-up, so besided the 2020 dip and a few early gaps, there isn't much added value in further analyzing this indicator. Because of that, we are going to dissect other ones. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46586c73",
   "metadata": {},
   "source": [
    "___\n",
    "## Density Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_nom_francais.copy()\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "\n",
    "# --- Compute total birds per transect per year ---\n",
    "transect_year_counts = (\n",
    "    df.groupby([\"year\", \"Nom transect\"])[\"TOT_AV_V\"]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"TOT_AV_V\": \"Bird_count\"})\n",
    ")\n",
    "\n",
    "# --- Normalize counts per year ---\n",
    "transect_year_counts[\"Normalized_density\"] = transect_year_counts.groupby(\"year\")[\"Bird_count\"].transform(\n",
    "    lambda x: x / x.max()\n",
    ")\n",
    "# --- color ---\n",
    "transects = sorted(transect_year_counts[\"Nom transect\"].unique())\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(transects)))  # up to 20 distinct colors\n",
    "color_map = dict(zip(transects, colors))\n",
    "\n",
    "# --- Plot normalized density distributions per year ---\n",
    "years = sorted(transect_year_counts[\"year\"].dropna().unique())\n",
    "\n",
    "for yr in years:\n",
    "    subset = transect_year_counts[transect_year_counts[\"year\"] == yr]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for _, row in subset.iterrows():\n",
    "        plt.bar(\n",
    "            row[\"Nom transect\"], \n",
    "            row[\"Normalized_density\"], \n",
    "            color=color_map[row[\"Nom transect\"]],\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5\n",
    "        )\n",
    "    plt.title(f\"Normalized bird observation density per transect ‚Äî {yr}\")\n",
    "    plt.xlabel(\"Transect\")\n",
    "    plt.ylabel(\"Normalized density (0‚Äì1)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4172b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "\n",
    "# --- Compute total birds per transect per year ---\n",
    "transect_year_counts = (\n",
    "    df.groupby([\"year\", \"Nom transect\"])[\"TOT_AV_V\"]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"TOT_AV_V\": \"Bird_count\"})\n",
    ")\n",
    "\n",
    "# --- Normalize globally (using the all-time maximum count) ---\n",
    "global_max = transect_year_counts[\"Bird_count\"].max()\n",
    "transect_year_counts[\"Normalized_density\"] = transect_year_counts[\"Bird_count\"] / global_max\n",
    "\n",
    "# --- Assign a consistent color per transect ---\n",
    "transects = sorted(transect_year_counts[\"Nom transect\"].unique())\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(transects)))  # up to 20 distinct colors\n",
    "color_map = dict(zip(transects, colors))\n",
    "\n",
    "# --- Plot evolution by year ---\n",
    "years = sorted(transect_year_counts[\"year\"].dropna().unique())\n",
    "\n",
    "for yr in years:\n",
    "    subset = transect_year_counts[transect_year_counts[\"year\"] == yr].sort_values(\"Normalized_density\", ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for _, row in subset.iterrows():\n",
    "        plt.bar(\n",
    "            row[\"Nom transect\"], \n",
    "            row[\"Normalized_density\"], \n",
    "            color=color_map[row[\"Nom transect\"]],\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5\n",
    "        )\n",
    "    \n",
    "    plt.title(f\"Bird density per transect ‚Äî normalized (global max=1) ‚Äî {yr}\")\n",
    "    plt.xlabel(\"Transect\")\n",
    "    plt.ylabel(\"Normalized density (0‚Äì1)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9b0b2",
   "metadata": {},
   "source": [
    "## Bootstrap Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f28daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters ---\n",
    "B = 1000  # number of bootstrap samples\n",
    "alpha = 0.05  # for 95% CI\n",
    "\n",
    "# --- Helper: bootstrap function ---\n",
    "def bootstrap_ci(data, func=np.mean, B=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence interval for a given statistic (func).\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    if n == 0:\n",
    "        return np.nan, np.nan, np.nan  # handle empty groups\n",
    "    \n",
    "    # Original estimate\n",
    "    theta_hat = func(data)\n",
    "    \n",
    "    # Bootstrap resamples\n",
    "    boot_estimates = []\n",
    "    for _ in range(B):\n",
    "        sample = np.random.choice(data, size=n, replace=True)\n",
    "        boot_estimates.append(func(sample))\n",
    "    boot_estimates = np.array(boot_estimates)\n",
    "    \n",
    "    # Quantiles of bootstrap distribution\n",
    "    q_low = np.quantile(boot_estimates, alpha / 2)\n",
    "    q_high = np.quantile(boot_estimates, 1 - alpha / 2)\n",
    "    \n",
    "    # Reflected confidence interval (percentile method)\n",
    "    ci_low = 2 * theta_hat - q_high\n",
    "    ci_high = 2 * theta_hat - q_low\n",
    "    \n",
    "    return theta_hat, ci_low, ci_high\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute bootstrap estimates per transect-year ---\n",
    "bootstrap_results = []\n",
    "for (year, transect), group in df.groupby([\"year\", \"Nom transect\"]):\n",
    "    theta_hat, ci_low, ci_high = bootstrap_ci(group[\"TOT_AV_V\"].values, func=np.sum, B=1000, alpha=0.05)\n",
    "    bootstrap_results.append({\n",
    "        \"year\": year,\n",
    "        \"transect\": transect,\n",
    "        \"total_birds\": theta_hat,\n",
    "        \"ci_low\": ci_low,\n",
    "        \"ci_high\": ci_high\n",
    "    })\n",
    "\n",
    "bootstrap_df = pd.DataFrame(bootstrap_results)\n",
    "\n",
    "# --- Normalize by global maximum total (for comparable densities) ---\n",
    "global_max = bootstrap_df[\"total_birds\"].max()\n",
    "bootstrap_df[\"density_norm\"] = bootstrap_df[\"total_birds\"] / global_max\n",
    "bootstrap_df[\"ci_low_norm\"] = bootstrap_df[\"ci_low\"] / global_max\n",
    "bootstrap_df[\"ci_high_norm\"] = bootstrap_df[\"ci_high\"] / global_max\n",
    "\n",
    "display(bootstrap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6109305",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(bootstrap_df[\"year\"].unique())\n",
    "\n",
    "for yr in years:\n",
    "    subset = bootstrap_df[bootstrap_df[\"year\"] == yr].sort_values(\"density_norm\", ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(\n",
    "        subset[\"transect\"],\n",
    "        subset[\"density_norm\"],\n",
    "        yerr=[subset[\"density_norm\"] - subset[\"ci_low_norm\"], subset[\"ci_high_norm\"] - subset[\"density_norm\"]],\n",
    "        capsize=3\n",
    "    )\n",
    "    plt.title(f\"Bootstrap-estimated normalized bird density per transect ‚Äî {yr}\")\n",
    "    plt.xlabel(\"Transect\")\n",
    "    plt.ylabel(\"Normalized density (0‚Äì1)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8159e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Identify top 5 transects by mean normalized density across all years ---\n",
    "top_transects = (\n",
    "    bootstrap_df.groupby(\"transect\")[\"density_norm\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(5)\n",
    "    .index\n",
    ")\n",
    "\n",
    "print(\"Top 5 transects with highest mean normalized density:\")\n",
    "print(top_transects.tolist())\n",
    "\n",
    "# --- Filter data for only those transects ---\n",
    "top_df = bootstrap_df[bootstrap_df[\"transect\"].isin(top_transects)]\n",
    "\n",
    "# --- Plot temporal evolution for top 5 transects ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "for transect, group in top_df.groupby(\"transect\"):\n",
    "    plt.plot(group[\"year\"], group[\"density_norm\"], marker=\"o\", label=transect)\n",
    "\n",
    "plt.title(\"Temporal evolution of normalized bird density ‚Äî Top 5 transects (after bootstrapping)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Normalized density (0‚Äì1)\")\n",
    "plt.legend(title=\"Transect\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b0ec58",
   "metadata": {},
   "source": [
    "### Shannon and simpson diversity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b715fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Aggregate counts per species per transect per year ---\n",
    "species_counts = (\n",
    "    df.groupby([\"year\", \"Nom transect\", \"ESPECE\"])[\"TOT_AV_V\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"TOT_AV_V\": \"count\"})\n",
    ")\n",
    "\n",
    "# --- Step 2: Define diversity index functions ---\n",
    "def shannon_index(values):\n",
    "    values = np.array(values)\n",
    "    values = values[values > 0]\n",
    "    proportions = values / values.sum()\n",
    "    return -np.sum(proportions * np.log(proportions + 1e-12))\n",
    "\n",
    "def simpson_index(values):\n",
    "    values = np.array(values)\n",
    "    values = values[values > 0]\n",
    "    proportions = values / values.sum()\n",
    "    return 1 - np.sum(proportions ** 2)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 3Ô∏è‚É£ After bootstrapping ‚Äî using bootstrap_df\n",
    "# ===========================\n",
    "boot_diversity = []\n",
    "for year, group in bootstrap_df.groupby(\"year\"):\n",
    "    vals = group[\"density_norm\"].values\n",
    "    sh_mean, sh_low, sh_high = bootstrap_ci(vals, shannon_index, B=1000)\n",
    "    si_mean, si_low, si_high = bootstrap_ci(vals, simpson_index, B=1000)\n",
    "    boot_diversity.append({\n",
    "        \"year\": year,\n",
    "        \"Shannon_boot\": sh_mean,\n",
    "        \"Shannon_low\": sh_low,\n",
    "        \"Shannon_high\": sh_high,\n",
    "        \"Simpson_boot\": si_mean,\n",
    "        \"Simpson_low\": si_low,\n",
    "        \"Simpson_high\": si_high\n",
    "    })\n",
    "boot_diversity = pd.DataFrame(boot_diversity)\n",
    "\n",
    "# --- Plot Shannon index comparison ---\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(boot_diversity[\"year\"], boot_diversity[\"Shannon_boot\"], \"o-\", label=\"Bootstrap Shannon (mean)\")\n",
    "plt.fill_between(boot_diversity[\"year\"], boot_diversity[\"Shannon_low\"], boot_diversity[\"Shannon_high\"], alpha=0.3, label=\"95% CI\")\n",
    "plt.title(\"Shannon diversity of transect densities ‚Äî raw vs bootstrap (with CI)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Shannon index (spatial diversity)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Simpson index comparison ---\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(boot_diversity[\"year\"], boot_diversity[\"Simpson_boot\"], \"s-\", label=\"Bootstrap Simpson (mean)\")\n",
    "plt.fill_between(boot_diversity[\"year\"], boot_diversity[\"Simpson_low\"], boot_diversity[\"Simpson_high\"], alpha=0.3, label=\"95% CI\")\n",
    "plt.title(\"Simpson diversity of transect densities ‚Äî raw vs bootstrap (with CI)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Simpson index (spatial diversity)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c6ecc",
   "metadata": {},
   "source": [
    "## Linear regression of density + P-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean density per year\n",
    "mean_density = (\n",
    "    transect_year_counts.groupby(\"year\")[\"Normalized_density\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values(\"year\")\n",
    ")\n",
    "\n",
    "# Define variables for regression\n",
    "X = sm.add_constant(mean_density[\"year\"])  # add intercept\n",
    "y = mean_density[\"Normalized_density\"]\n",
    "\n",
    "# Fit the OLS (Ordinary Least Squares) regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Extract the p-value for the year coefficient\n",
    "p_value = model.pvalues[\"year\"]\n",
    "slope = model.params[\"year\"]\n",
    "r_squared = model.rsquared\n",
    "\n",
    "print(f\"Slope: {slope:.6f}\")\n",
    "print(f\"p-value (year effect): {p_value:.6f}\")\n",
    "print(f\"R¬≤: {r_squared:.4f}\")\n",
    "\n",
    "# Predict fitted values and confidence interval\n",
    "predictions = model.get_prediction(X)\n",
    "pred_summary = predictions.summary_frame(alpha=0.05)\n",
    "\n",
    "# Plot data + regression line + 95% confidence interval\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(mean_density[\"year\"], mean_density[\"Normalized_density\"], label=\"Observed mean densities\")\n",
    "plt.plot(mean_density[\"year\"], pred_summary[\"mean\"], label=\"Fitted regression line\", linewidth=2)\n",
    "plt.fill_between(\n",
    "    mean_density[\"year\"],\n",
    "    pred_summary[\"mean_ci_lower\"],\n",
    "    pred_summary[\"mean_ci_upper\"],\n",
    "    alpha=0.3,\n",
    "    label=\"95% confidence interval\"\n",
    ")\n",
    "plt.title(\"Linear regression of mean normalized density over years\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Mean normalized density\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca2a1b",
   "metadata": {},
   "source": [
    "# Summary of Density indicator (what we did)\n",
    "\n",
    "## ü™∂ Density Indicator ‚Äî Summary of Computations\n",
    "\n",
    "### 1. Definition\n",
    "The density indicator measures the relative abundance of birds observed per transect and year.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Computation Steps\n",
    "\n",
    "**(a) Counting per Transect and Year**  \n",
    "From the cleaned observation dataset (`nom_francais_clean`), total bird counts were aggregated by `(year, transect)` using columns such as `TOT_A`, `TOT_V_sV`, etc.\n",
    "\n",
    "**(b) Normalization**  \n",
    "Each transect‚Äôs annual count was normalized by the maximum count observed across all years:\n",
    "\n",
    "$$\n",
    "\\text{density\\_norm}_{i,t} = \\frac{\\text{count}_{i,t}}{\\max(\\text{count}_{\\text{all years}})}\n",
    "$$\n",
    "\n",
    "Densities are thus scaled to the range [0, 1].\n",
    "\n",
    "\n",
    "\n",
    "### 3. Bootstrap Estimation\n",
    "\n",
    "A bootstrap resampling method was used to estimate uncertainty:\n",
    "\n",
    "1. For each year, resample transects with replacement \\( B \\) times (e.g. \\( B = 1000 \\)).\n",
    "2. Compute the mean normalized density for each resample.\n",
    "3. Obtain 95% confidence intervals from the empirical quantiles of the bootstrap distribution.\n",
    "\n",
    "$$\n",
    "\\text{CI}_{95\\%} = [\\hat{\\theta}^*_{2.5\\%}, \\hat{\\theta}^*_{97.5\\%}]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 4. Derived Indicators and Visualization\n",
    "\n",
    "- Annual mean normalized density computed and plotted over time.  \n",
    "- Per-year density distribution visualized by transect (color-coded).  \n",
    "- Temporal evolution of normalized densities visualized across transects.  \n",
    "- Bootstrap mean and confidence intervals plotted for density trends.\n",
    "\n",
    "\n",
    "### 5. Diversity Indices Applied to Density\n",
    "\n",
    "**Shannon Diversity Index**\n",
    "$$\n",
    "H' = -\\sum_i p_i \\ln(p_i)\n",
    "$$\n",
    "\n",
    "**Simpson Diversity Index**\n",
    "$$\n",
    "D = 1 - \\sum_i p_i^2\n",
    "$$\n",
    "\n",
    "Both indices computed from the normalized density distribution across transects:\n",
    "- Calculated per year on raw and bootstrapped densities.\n",
    "- Confidence intervals estimated via bootstrap resampling.\n",
    "- Temporal evolution of both indices visualized.\n",
    "- Additional plots generated for the top 5 transects with highest mean density.\n",
    "\n",
    "\n",
    "### 6. Linear Regression and p-Value Computation\n",
    "\n",
    "A **linear regression** was fitted using `statsmodels` to assess the presence of a temporal trend in mean normalized density:\n",
    "\n",
    "$$\n",
    "\\text{density\\_norm} = \\beta_0 + \\beta_1 \\times \\text{year} + \\varepsilon\n",
    "$$\n",
    "\n",
    "- The regression was estimated with the **Ordinary Least Squares (OLS)** method.  \n",
    "- The **slope** (\\(\\beta_1\\)) quantifies the direction and magnitude of the density trend over years.  \n",
    "- The **p-value** for the `year` coefficient tests the null hypothesis \\(H_0: \\beta_1 = 0\\).  \n",
    "- A **95% confidence interval** and **R¬≤** statistic were also extracted to evaluate model fit.  \n",
    "- The fitted regression line and its 95% confidence band were plotted together with observed mean densities.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be258cc",
   "metadata": {},
   "source": [
    "___\n",
    "## Detectability Study (auditory vs visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99357ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 5  # number of transects we analyze (ordered by most visited)\n",
    "\n",
    "\n",
    "df_det = clean_nom_francais.copy()\n",
    "df_det[\"date\"] = pd.to_datetime(df_det[\"date\"], errors=\"coerce\")\n",
    "df_det[\"year\"] = pd.to_numeric(df_det[\"date\"].dt.year, errors=\"coerce\")\n",
    "df_det[\"N¬∞ passage\"] = pd.to_numeric(df_det[\"N¬∞ passage\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "df_det = df_det.dropna(subset=[\"Nom transect\", \"year\"])\n",
    "\n",
    "\n",
    "ranking = df_det.groupby(\"Nom transect\").size().sort_values(ascending=False)\n",
    "top_transects = ranking.head(TOP_N).index.tolist() \n",
    "df_det_top = df_det[df_det[\"Nom transect\"].isin(top_transects)]\n",
    "\n",
    "# -------- AGGREGATE & SHARES --------\n",
    "agg = (df_det_top.groupby([\"Nom transect\", \"year\"], dropna=False)\n",
    "              .agg(A_sum=(\"TOT_A\",\"sum\"), V_sum=(\"TOT_V_sV\",\"sum\"))\n",
    "              .reset_index())\n",
    "total = (agg[\"A_sum\"] + agg[\"V_sum\"])\n",
    "agg[\"Auditory_pct\"] = 100 * agg[\"A_sum\"] / total\n",
    "agg[\"Visual_pct\"]   = 100 - agg[\"Auditory_pct\"]\n",
    "\n",
    "# Plot the histogram\n",
    "n = len(top_transects)\n",
    "ncols = min(3, n)\n",
    "nrows = int(np.ceil(n / ncols))\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 10), squeeze=False)\n",
    "\n",
    "for i, tr in enumerate(top_transects):\n",
    "    ax = axes[i // ncols, i % ncols]\n",
    "    sub = agg[agg[\"Nom transect\"] == tr].sort_values(\"year\")\n",
    "    years = sub[\"year\"].astype(int).tolist()\n",
    "    a = sub[\"Auditory_pct\"].values\n",
    "    v = sub[\"Visual_pct\"].values\n",
    "\n",
    "    ax.bar(years, a, label=\"Auditory (%)\")\n",
    "    ax.bar(years, v, bottom=a, label=\"Visual (%)\")\n",
    "    ax.set_title(f\"{tr} ‚Äî Modality mix\")\n",
    "    ax.set_xlabel(\"Year\"); ax.set_ylabel(\"Share (%)\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xticks(years); ax.set_xticklabels(years, rotation=45)\n",
    "\n",
    "# Hide empty axes\n",
    "for j in range(i+1, nrows*ncols):\n",
    "    axes[j // ncols, j % ncols].axis(\"off\")\n",
    "\n",
    "handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper center\", ncol=2, bbox_to_anchor=(0.5, 1.03))\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75b41b",
   "metadata": {},
   "source": [
    "Observing the graphs for each of the top five most visited transects, we can see that for some of them `(Morne Babet, Habitation Petit Riviere, Moulin a vent)`, there seems to be an increase in the share of auditory observations of bird. \n",
    "\n",
    "In order to test our hypothesis if there are trends in some of these transects, we are going to present an initial hypothesis $H_0$: $\\beta_1 = 0$, with $\\beta_1$ being the slope on each year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d346b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "agg[\"AV_total\"] = (agg[\"A_sum\"] + agg[\"V_sum\"]).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "ci_low, ci_high = proportion_confint(\n",
    "    count=agg[\"A_sum\"].astype(float),\n",
    "    nobs=agg[\"AV_total\"].astype(float),\n",
    "    alpha=0.05,\n",
    "    method=\"wilson\"\n",
    ")\n",
    "agg.loc[\"Auditory_pct_low\"]  = 100 * ci_low\n",
    "agg.loc[\"Auditory_pct_high\"] = 100 * ci_high\n",
    "\n",
    "# We fit a binomial GLM for the % auditory ~ year (per transect)\n",
    "def fit_binom_glm(sub_df):\n",
    "    \"\"\"sub_df has columns: year, A_sum, V_sum, AV_total (>0 rows). Returns pred df + slope, pval.\"\"\"\n",
    "    df = sub_df.sort_values(\"year\").copy()\n",
    "\n",
    "    df[\"year_c\"] = df[\"year\"] - df[\"year\"].mean()  \n",
    "    y = (df[\"A_sum\"] / df[\"AV_total\"]).values\n",
    "    X = sm.add_constant(df[[\"year_c\"]])\n",
    "    model = sm.GLM(y, X, family=sm.families.Binomial(), var_weights=df[\"AV_total\"].values)\n",
    "    res = model.fit(cov_type=\"HC1\")\n",
    "\n",
    "    pred = res.get_prediction(X)\n",
    "    ci = pred.conf_int(alpha=0.05)\n",
    "    df[\"pred\"]   = pred.predicted_mean\n",
    "    df[\"ci_low\"] = ci[:, 0]\n",
    "    df[\"ci_high\"]= ci[:, 1]\n",
    "\n",
    "    slope = float(res.params.get(\"year_c\"))  \n",
    "    pval  = float(res.pvalues.get(\"year_c\"))\n",
    "    return df, slope, pval\n",
    "\n",
    "# We plot the observed auditory (%) bars with Wilson CIs and GLM fit\n",
    "transects = top_transects\n",
    "n = len(transects)\n",
    "ncols = min(3, n)\n",
    "nrows = int(np.ceil(n / ncols)) if n else 1\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 10), squeeze=False)\n",
    "axes = axes.flatten() if nrows*ncols > 1 else [axes]\n",
    "\n",
    "for i, tr in enumerate(transects):\n",
    "    ax = axes[i]\n",
    "    sub = agg[agg[\"Nom transect\"] == tr].sort_values(\"year\").copy()\n",
    "    years = sub[\"year\"].astype(int).to_numpy()\n",
    "    y_bar = sub[\"Auditory_pct\"].to_numpy()\n",
    "\n",
    "    # Bars: observed auditory %\n",
    "    ax.bar(years, y_bar, label=\"Observed Auditory (%)\", alpha=0.85)\n",
    "\n",
    "    # Wilson error bars (only where defined)\n",
    "    y_low  = sub[\"Auditory_pct_low\"].to_numpy()\n",
    "    y_high = sub[\"Auditory_pct_high\"].to_numpy()\n",
    "    yerr = np.vstack([y_bar - y_low, y_high - y_bar])\n",
    "    ax.errorbar(years, y_bar, yerr=yerr, fmt=\"none\", capsize=3, linewidth=1, ecolor=\"black\")\n",
    "\n",
    "    # GLM fit line + 95% CI band\n",
    "    fit_df, slope, pval = fit_binom_glm(sub)\n",
    "    ax.plot(fit_df[\"year\"].astype(int), fit_df[\"pred\"]*100, marker=\"o\",\n",
    "                linestyle=\"-\", linewidth=1.5, label=\"Binomial fit\")\n",
    "    ax.fill_between(fit_df[\"year\"].astype(int),\n",
    "                    fit_df[\"ci_low\"]*100, fit_df[\"ci_high\"]*100,\n",
    "                    alpha=0.20, label=\"95% CI (fit)\")\n",
    "\n",
    "    ax.set_title(f\"{tr} ‚Äî Observed Auditory (%)\\nlogit-slope={slope:.3f}, p={pval:.3g}\")\n",
    "    ax.set_xlabel(\"Year\"); ax.set_ylabel(\"Auditory (%)\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xticks(years); ax.set_xticklabels(years, rotation=45)\n",
    "\n",
    "# Hide unused subplots \n",
    "for j in range(i + 1, nrows * ncols):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "if handles:\n",
    "    fig.legend(handles, labels, loc=\"upper center\", ncol=3, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18d6ba",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "After tracing these figures, with their corresponding CIs, p_values and slopes, we can see that for: \n",
    "1. Morne Babet: The p_value is much smaller than 0.05. We can say that the trend is highly significant, and that there is a strong and steady increase in the proportion of auditory detections over time.\n",
    "2. Habitation Petit Rivi√®re: Same as Morne Babet\n",
    "3. Moulin √† Vent: The p_value is smaller than 0.05. We can say that the trend is signicant, and that there is more variability across the years, but the fitted line still shows a positive slope.\n",
    "4. L√†-Haut: The p_value is much bigger than 0.05. We can say that the trend is not significant, and that the modality mix remains roughly constant over time.\n",
    "5. H√¥tel des Plaisirs: the p_value is bigger than 0.05. We can say that the trend is not significant, and although the slope is slightly upward, the uncertainty is large, so there isn't any statistically significant trend in auditory share.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0eaf3",
   "metadata": {},
   "source": [
    "# Summary of Modality-Mix Indicator (Auditory vs Visual)\n",
    "\n",
    "## 1. Definition\n",
    "The modality-mix indicator measures, for each transect and year, the share of detections that were auditory and visual:\n",
    "\n",
    "$$\n",
    "\\frac{A_{y,t}}{A_{y,t} + V_{y,t}}\n",
    "$$\n",
    "\n",
    "where $A_{y,t}=\\text{auditory detections}$ and $V_{y, t} = \\text{visual detections}$ for each year $y$ at transect $t$.\n",
    "\n",
    "## 2. Computation Steps\n",
    "\n",
    "**(a) Counting per Transect and Year**\n",
    "\n",
    "From the cleaned observation dataset `(nom_francais_clean)`, total audio/visual bird counts were aggregated by `(year, transect)` adding columns `TOT_A` and `TOT_V_sV`.\n",
    "\n",
    "**(b) Observed shares**\n",
    "\n",
    "To calculate the percentage of observed audio shares, we do:\n",
    "$$\n",
    "\\widehat{p}_{y,t}=\\frac{TOT\\_A}{AV\\_total},\\qquad \\text{Auditory\\%}=100\\times \\widehat{p}_{y,t}\n",
    "$$\n",
    "\n",
    "## 3. Confidence Intervals for Observed Shares (Wilson method)\n",
    "\n",
    "We use the Wilson method, because classic \"Wald\" Confidence Intervalls perform poorly with small $n$ or extreme $p$ (bounds can leave [0, 1]) \n",
    "Instea, Wilson score intervals have better coverage and stay in [0, 1], which matters because if we increase the number of transects, the $\\text{year} \\times \\text{transect}$ might have modest totals.\n",
    "\n",
    "Formula for $95\\%$ CI:\n",
    "\n",
    "Let $z = 1.96$, $n = \\text{AV\\_total}$, $\\widehat{p} = \\frac{\\text{TOT\\_A}}{n}$. Then\n",
    "\n",
    "$$\n",
    "\\text{CI}_{\\text{Wilson}} = \\frac{\\widehat{p} + \\frac{z^2}{2n} \\pm z\\sqrt{\\frac{\\widehat{p}(1 - \\widehat{p})}{n} + \\frac{z^2}{4n^2}}}{1 + \\frac{z^2}{n}}\n",
    "$$\n",
    "\n",
    "In the code, this is calculated by the imported function `statsmodels.stats.proportion.proportion_confint(method=\"wilson\")`, then we multiply it by 100 to get the percentage.\n",
    "\n",
    "## 4. Binomial Generalized Linear Model\n",
    "\n",
    "We use a GLM because the share of auditory detections is a proportion, each value comes from a count of auditory detections out of a total number of detections, so it follows a binomial distribution. Also, a GLM with a binomial family and a logit link models how the probability of an auditory detection changes with time (year), while taking into account that the years with more detections provide more reliable estimates. \n",
    "\n",
    "Per each transect, we utilise the following model:\n",
    "\n",
    "$$\n",
    "\\text{logit}(\\text{Pr}[\\text{auditory}|y,t]) = \\beta_0 + \\beta_1 (y - \\widehat{y})\n",
    "$$\n",
    "\n",
    "With:\n",
    "1. $\\beta_0$: intercept\n",
    "2. $\\beta_1$: slope on year\n",
    "3. $(y-\\hat y)$: centered year\n",
    "\n",
    "To calculate the p_value $p$ of each transect, we are going to do the following computations:\n",
    "\n",
    "$$\n",
    "z = \\frac{\\hat \\beta_1}{\\text{SE}(\\hat \\beta_1)d},\\qquad p=2(1 - \\phi(|z|))\n",
    "$$\n",
    "\n",
    "Where $\\phi(\\cdot)$ is the standard normal CDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a79f4",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute total abundance per species\n",
    "species_counts = (\n",
    "    df.groupby(\"ESPECE\")[\"TOT_AV_V\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "top5_species = species_counts.head(5).index.tolist()\n",
    "print(\"Top 5 most observed species:\", top5_species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_data = (\n",
    "    df[df[\"ESPECE\"].isin(top5_species)]\n",
    "    .groupby([\"year\", \"ESPECE\"])[\"TOT_AV_V\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff81d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for species in top5_species:\n",
    "    sub = top5_data[top5_data[\"ESPECE\"] == species]\n",
    "    plt.plot(sub[\"year\"], sub[\"TOT_AV_V\"], marker=\"o\", label=species)\n",
    "\n",
    "plt.title(\"Temporal Evolution of Abundance for Top 5 Most Observed Bird Species\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Recorded Abundance (TOT_AV_V)\")\n",
    "plt.legend(title=\"Species\", bbox_to_anchor=(1.05,1), loc=\"upper left\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35611cc1",
   "metadata": {},
   "source": [
    "### Normalization to compare trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_data_norm = top5_data.copy()\n",
    "top5_data_norm[\"scaled\"] = top5_data_norm.groupby(\"ESPECE\")[\"V_V\"].transform(\n",
    "    lambda s: (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for species in top5_species:\n",
    "    sub = top5_data_norm[top5_data_norm[\"ESPECE\"] == species]\n",
    "    plt.plot(sub[\"year\"], sub[\"scaled\"], marker=\"o\", label=species)\n",
    "\n",
    "plt.title(\"Normalized Temporal Trends of Top 5 Bird Species (0‚Äì1 scale)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Relative abundance (normalized)\")\n",
    "plt.legend(title=\"Species\", bbox_to_anchor=(1.05,1), loc=\"upper left\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c929e80",
   "metadata": {},
   "source": [
    "### Bootstrap + CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_results = []\n",
    "\n",
    "for species in top5_species:\n",
    "    sp = df[df[\"ESPECE\"] == species]\n",
    "    \n",
    "    for year, group in sp.groupby(\"year\"):\n",
    "        counts = group[\"TOT_AV_V\"].values\n",
    "        \n",
    "        mean, ci_low, ci_high = bootstrap_ci(counts,func=np.mean, B=1000, alpha=0.05)\n",
    "        \n",
    "        bootstrap_results.append({\n",
    "            \"species\": species,\n",
    "            \"year\": year,\n",
    "            \"mean_abundance\": mean,\n",
    "            \"ci_low\": ci_low,\n",
    "            \"ci_high\": ci_high\n",
    "        })\n",
    "\n",
    "bootstrap_df_species = pd.DataFrame(bootstrap_results).sort_values([\"species\",\"year\"])\n",
    "bootstrap_df_species.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for species, group in bootstrap_df_species.groupby(\"species\"):\n",
    "    plt.plot(group[\"year\"], group[\"mean_abundance\"], marker=\"o\", label=species)\n",
    "    plt.fill_between(group[\"year\"], group[\"ci_low\"], group[\"ci_high\"], alpha=0.2)\n",
    "\n",
    "plt.title(\"Temporal Evolution of Bird Abundance (Top 5 Species) with Bootstrap 95% CI\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Mean Recorded Abundance (TOT_AV_V)\")\n",
    "plt.legend(title=\"Species\", bbox_to_anchor=(1.05,1), loc=\"upper left\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c24098",
   "metadata": {},
   "outputs": [],
   "source": [
    "for species, group in bootstrap_df_species.groupby(\"species\"):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(group[\"year\"], group[\"mean_abundance\"], marker=\"o\")\n",
    "    plt.fill_between(group[\"year\"], group[\"ci_low\"], group[\"ci_high\"], alpha=0.3)\n",
    "    plt.title(f\"Annual Abundance of {species} (with 95% Bootstrap CI)\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Mean Recorded Abundance (TOT_AV_V)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46568de2",
   "metadata": {},
   "source": [
    "## Trend Significance per Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_results = []\n",
    "\n",
    "for species, group in bootstrap_df_species.groupby(\"species\"):\n",
    "    \n",
    "    # Sort by year to ensure correct alignment\n",
    "    group = group.sort_values(\"year\")\n",
    "    \n",
    "    # Linear regression test\n",
    "    X = sm.add_constant(group[\"year\"])   # predictor: year\n",
    "    y = group[\"mean_abundance\"]         # response: bootstrapped mean abundance\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    slope = model.params[\"year\"]\n",
    "    p_slope = model.pvalues[\"year\"]\n",
    "    r2 = model.rsquared\n",
    "    \n",
    "    # Kendall Tau Non-parametric trend test\n",
    "    tau, p_kendall = kendalltau(group[\"year\"], group[\"mean_abundance\"])\n",
    "    \n",
    "    trend_results.append({\n",
    "        \"species\": species,\n",
    "        \"slope (OLS)\": slope,\n",
    "        \"p-value (OLS slope)\": p_slope,\n",
    "        \"R¬≤ (trend fit)\": r2,\n",
    "        \"Kendall Tau\": tau,\n",
    "        \"p-value (Kendall)\": p_kendall\n",
    "    })\n",
    "\n",
    "trend_results_df = pd.DataFrame(trend_results)\n",
    "trend_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36652b69",
   "metadata": {},
   "source": [
    "For all this species except \"Quiscale merle\" , the slope is > 0 , so species abundance is increasing over time, but the only significant values are for \"El√©nie siffleuse\" and \t\"Tourterelle √† queue carr√©e\". \"Quiscale merle\" has also a significant negative value, so could suppose that the abundance is decreasing over the time, but the p-value is quite high. This suggests a possible decline, but evidence is weak. By looking at p-value (< 0.05), we can say that trend is statistically significant for \"El√©nie siffleuse\" and \"Tourterelle √† queue carr√©e\". Looking at Kendall p-value (< 0.05) trend is significant even without assuming linearity for these two species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67678b",
   "metadata": {},
   "source": [
    "### Trend Line + Bootstrap CI + Observed Points (per species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd96af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for species, group in bootstrap_df_species.groupby(\"species\"):\n",
    "    \n",
    "    group = group.sort_values(\"year\")  # ensure correct order\n",
    "    years = group[\"year\"].values\n",
    "    y = group[\"mean_abundance\"].values\n",
    "    \n",
    "    # Fit regression for the species\n",
    "    X = sm.add_constant(years)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # Predictions + Confidence Interval for trend line\n",
    "    pred = model.get_prediction(X).summary_frame(alpha=0.05)\n",
    "    y_fit = pred[\"mean\"]\n",
    "    y_low = pred[\"mean_ci_lower\"]\n",
    "    y_high = pred[\"mean_ci_upper\"]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,5))\n",
    "    \n",
    "    # Bootstrap CI (vertical uncertainty)\n",
    "    plt.fill_between(years, group[\"ci_low\"], group[\"ci_high\"], alpha=0.25, label=\"Bootstrap 95% CI (Abundance)\")\n",
    "    \n",
    "    # Observed points\n",
    "    plt.plot(years, y, marker=\"o\", linewidth=1.5, label=\"Mean Recorded Abundance\")\n",
    "    \n",
    "    # Linear regression trend\n",
    "    plt.plot(years, y_fit, linewidth=2.2, label=\"Fitted Trend (OLS)\")\n",
    "    \n",
    "    # Confidence band for trend\n",
    "    plt.fill_between(years, y_low, y_high, alpha=0.2, label=\"95% CI (Trend)\")\n",
    "    \n",
    "    plt.title(f\"Trend in Annual Abundance ‚Äî {species}\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Mean Abundance (TOT_AV_V)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b8319",
   "metadata": {},
   "source": [
    "### Interpretation of Species-Specific Abundance Trends\n",
    "\n",
    "The figures above display the temporal evolution of abundance for the five most frequently recorded bird species in the dataset, using the standardized mean abundance (TOT_AV_V) per year and associated uncertainty estimated via bootstrap resampling. For **√âlenie siffleuse**, the annual mean abundance shows a generally increasing pattern over the study period, and the fitted linear trend is positive with confidence intervals that do not overlap heavily with zero, indicating a statistically supported rise in abundance. A similar positive and statistically significant trend is observed for **Tourterelle √† queue carr√©e**, where both the slope and the bootstrap confidence intervals suggest a sustained increase in occurrence intensity over time. \n",
    "\n",
    "In contrast, **Quiscale merle** shows a declining fitted trend line, but the year-to-year variability is relatively high and the confidence intervals are broader, resulting in a non-significant trend. This suggests that although the species may be experiencing a reduction in recorded abundance, further data or more controlled sampling would be needed to confidently confirm this decline. For **Sporophile rougegorge** and **Sucrier √† ventre jaune**, the mean abundance values fluctuate from year to year without displaying a marked directional change. Their fitted trends are near-flat and associated confidence intervals are wide, indicating that these populations have remained relatively stable across the studied period.\n",
    "\n",
    "Overall, these results highlight **two species experiencing significant increases** (√âlenie siffleuse and Tourterelle √† queue carr√©e), **one species with a possible but unconfirmed decline** (Quiscale merle), and **two species showing stable abundance levels with no evidence of directional long-term change** (Sporophile rougegorge and Sucrier √† ventre jaune).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46618ede",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "The mean-abundance and bootstrap approach provides a robust, effort-standardized indicator of species presence over time. However, this method does not explicitly correct for variation in detection probability (observer effects, weather, time of day), and assumes independence between sampling events, which may not always hold. Additionally, mean abundance reflects relative observation rates rather than absolute population sizes, because mean abundance is how often the species are recorded, not how many individuals exist in the ecosystem., and the linear trend model may not capture non-linear ecological dynamics. Therefore, while the approach reliably identifies broad directional changes, results should be interpreted as population indices rather than direct population estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf7280",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
